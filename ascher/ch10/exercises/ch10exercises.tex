\documentclass[12pt,a4]{article}

\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{amscd}
\usepackage{mathtools}
\usepackage{geometry, algorithmicx} 
\usepackage[noend]{algpseudocode}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{pgf, tikz}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{arrows, automata}
\theoremstyle{definition}


\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[CE]{\Author}
\fancyhead[CO]{\Title}
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}

\author{Colin Ford}
\title{Ascher - Chapter 10 Exercises}
\date{}

\makeatletter
\let\Title\@title
\makeatother

\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem{problem}{Problem}
\newtheorem*{problem*}{Problem}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem*{definition*}{Definition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}
\newtheorem*{example}{Example}

\setcounter{exercise}{-1}

\begin{document}

\maketitle

% % % % % % % % % % %
% % % Exercise 0 % % %
% % % % % % % % % % %
\begin{exercise}[Review Questions]
	\begin{enumerate}[(a)]
		% % a % %
		\item Distinguish between the terms data fitting, interpolation, and polynomial interpolation.
		
		% % b % %
		\item Distinguish between (discrete) data fitting and approximating a given function.
		
		% % c % %
		\item What are basis functions? Does an approximant $v(x)$ that is written as a linear combination of basis functions have to be linear in $x$?
		
		% % d % %
		\item An interpolating polynomial is unique regardless of the choice of the basis. Explain why.
		
		% % e % %
		\item State one advantage and two disadvantages of using the monomial basis for polynomial interpolation.
		
		% % f % %
		\item What are Lagrange polynomials? How are they used for polynomial interpolation?
		
		% % g % %
		\item What are barycentric weights?
		
		% % h % %
		\item State the main advantages and the main disadvantage for using the Lagrange representation.

		% % i % %
		\item What is a divided difference table and how is it constructed?
		
		% % j % %
		\item Write down the formula for polynomial interpolation in Newton form.
		
		% % k % %
		\item State two advantages and two disadvantages for using the Newton representation for polynomial interpolation.
		
		% % l % %
		\item Describe the linear systems that are solved for the monomial basis, the Lagrange representation, and the Newton representation.
		
		% % m % %
		\item Describe the connection between the $k$th divided difference of a function $f$ and its $k$th derivative.
		
		% % n % %
		\item Provide an expression for the error in polynomial interpolation as well as an error bound expression.
		
		% % o % %
		\item How does the smoothness of a function and its derivatives affect the quality of polynomial interpolants that approximate it, in general?
		
		% % p % %
		\item Give an example where the error bound is attained. 
		
		% % q % %
		\item When we interpolate a function $f$ given only data points, i.e., we do not know $f$ or its derivatives, how can we gauge the accuracy of our approximation?
		
		% % r % %
		\item What are Chebyshev points and why are they important?
		
		% % s % %
		\item Describe osculating interpolation. How is it different from the usual polynomial interpolation?
		
		% % t % %
		\item What is a Hermite cubic interpolant?
		
	\end{enumerate}
\end{exercise}
\begin{proof}[Solution]
	\begin{enumerate}[(a)]
		% % a % %
		\item \emph{Data fitting} is the process of finding or constructing a reasonable function $v$ that fits a given set of data. \emph{Interpolation} is a method of constructing a reasonable function $v$ to fit data such that this function exactly passes through the given data. In other words, given a set of data points $\{ (x_i, y_i) \}_{i = 0}^n$, require that $v(x)$ \emph{interpolate} this data so that it satisfies 
		
		\[
		v(x_i) = y_i {,} \quad i = 0, 1 \ldots, n {.}
		\]		
		
		From this one can understand better why interpolation is a special case of approximation. \emph{Polynomial interpolation} is a special case of interpolation, where we restrict our search for an interpolated function $v$ to the polynomials, i.e., 
		
		\[
		v(x) = \sum_{j = 0}^{n} c_j x^j = c_0 + c_1 x^1 + \cdots + c_n x^n {.}
		\]
		
		By ``reasonable'', we mean (loosely) that the interpolating function must resemble a curve that we would actually draw through the points. By ``restrict our search'', we simply mean to restrict our choice of basis functions to some set of polynomials. 
		
		% % b % %
		\item As stated in (a), discrete data fitting is the process of, given a set of (discrete) data points $\{ (x_i, y_i) \}_{i = 0}^n$, finding a reasonable function $v(x)$ that fits the data points. Approximating a given function\footnote{Note that this given function could be given explicitly or implicitly.} $f(x)$ is the process of finding some simpler function $v(x)$ that approximates $f(x)$. The difference between these two methods is that, in approximating a function, we have some freedom of choosing the $x_i$ cleverly as well as that we may be able to consider the \emph{global} interpolation error. 
		
		It is easy to see how one may have more opportunity to consider the global interpolation error when approximating some given function since we may use global properties of the given function when approximating. On the other hand, when data fitting, we are merely given a set of data points which will allow us to construct a function which only \emph{locally} resembles the approximated data or function. 
		
		% % c % %
		\item If $V$ is a vector space over a field $k$, a \emph{linear form} $f$ is a linear function from $V$ to $k$, i.e.
		
		\begin{gather*}
		f(u + v) = f(u) + f(v) \quad \text{for all } u, v \in V \\
		f(a v) = a f(v) \quad \text{for all } a \in k \text{ and } v \in V {.}
		\end{gather*}
		
		We generally assume a linear form for all interpolating functions $v(x)$, and write 
		
		\[
		v(x) = \sum_{j = 0}^n c_j \phi_j(x) = c_0 \phi_0(x) + \cdots + c_n \phi_n(x) {,}
		\]
		
		where $\{ c_j \}_{j = 0}^n$ are \emph{unknown coefficients} or \emph{parameters} determined from the data and $\{ \phi_j (x) \}_{j  = 0}^n$ are predetermined \emph{basis functions}. These basis functions are assumed to be linearly independent. Notice that $v(x)$ is a linear form, so it is not necessarily linear in $x$; it is linear in its basis functions. This is a simple consequence of the definition. 
		
		% % d % %
		\item We give simple proof of the following theorem. 
		
		\begin{theorem*}
			Let $\{ x_i \}_{i = 0}^n$ be a sequence of distinct real numbers. Then for some arbitrary sequence $\{ y_i \}_{i = 0}^n$ of real numbers there exists a unique polynomial $p$ of degree $n$ such that 
			
			\[
			p(x_i) = y_i \quad \text{for } i = 0, \ldots, n {.}
			\]
		\end{theorem*}
		\begin{proof}
			Suppose there were another such polynomial $q$. Then the polynomial $p - q$ is of degree $n$ with zeros at $x_i$ for each $i$. Hence $p - q$ has $n + 1$ zeros, which is only possible if it is the zero polynomial. Hence $p \equiv q$. 
		\end{proof}
		
		% % e % %
		\item A major advantage of using a monomial basis is its intuitive simplicity and straightforwardness. Some disadvantages include: 
		
		\begin{itemize}
			\item the calculated coefficients $c_j$ are not directly indicative of the interpolated function, and they may completely change if we wish to slightly modify the interpolation problem
			
			\item the Vandermonde matrix $X$ is often ill-conditioned, so the coefficients thus determined are prone to inaccuracies 
			
			\item this approach requires about $\frac{2}{3} n^3$ operations (flops) to carry out Gaussian elimination for the construction stage; another method exists which requires only $O(n^2)$ operations. The evaluation stage, however, is as quick as can be; using the nested form, it requires about $2 n$ flops per evaluation point. 
		\end{itemize} 
		
		The latter two disadvantages are not always important. The $O(n^3)$ cost matters when $n$ is large. The ill-conditioning of the Vandermonde matrix matters mostly when the interval of interpolation or the size of $n$ is large. 
		
		% % f % %
		\item A \emph{Lagrange polynomial} $L_j(x)$ is a polynomial of degree $n$ that satisfies
		
		\[
		L_j(x_i) = \begin{cases}
		0 {,} &\quad i \neq j {,} \\
		1 {,} &\quad i = j {.}
		\end{cases}
		\]
		
		Lagrange polynomials are used as a basis in polynomial interpolation. By this we mean 
		
		\[
		p(x) = \sum_{j = 0}^{n} y_j L_j(x) {.} 
		\]
		
		% % g % %
		\item Barycentric weights are used in the construction process of Lagrange polynomial interpolation. Define
		
		\[
		\rho_j = \prod_{i \neq j} (x_j - x_i) {,} \quad w_j = \frac{1}{\rho_j} {,} \quad j = 0, 1, \ldots, n {.}
		\]
		
		This construction requires $O(n^2)$ flops, and the quantities $w_j$ are called \emph{barycentric weights}. 
		
		% % h % %
		\item The Lagrange polynomials form an ideally conditioned basis for all polynomials of degree at most $n$. Another advantage is that the Lagrange representation is easy to manipulate when seeking formulas for differentiation and integration. A disadvantage is that must introduce the interpolation data $(x_i, y_i)$ all at once instead of one pair at a time. 
		
		% % i % %
		\item The coefficient $c_j$ of the interpolating polynomial in Newton's form is called the $j$th \emph{divided difference}, denoted $f[x_0, x_1, \ldots, x_j]$. These are defined recursively as follows. Given points $x_0, x_1, \ldots, x_n$, for arbitrary indices $0 \leq i < j \leq n$, set 
		
		\begin{align*}
		f[x_i] &= f(x_i) {,} \\
		f[x_i, \ldots, x_j] &= \frac{f[x_{i + 1}, \ldots, x_j] - f[x_i, \ldots, x_{j - 1}]}{x_j - x_i} {.}
		\end{align*}
		
		A \emph{divided difference table} is a lower triangular matrix composed of the divided differences, where the diagonal entries are the coefficients $c_j = \gamma_{j, j} = f[x_0, \ldots, x_j]$. 
		
		% % j % %
		\item The formula is given by 
		
		\begin{align*}
		p_n(x) &= f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0) (x - x_1) \\
		 &\quad + \cdots + f[x_0, x_1, \ldots, x_n] (x - x_0) (x - x_1) \cdots (x - x_{n - 1}) \\
		 &= \sum_{j = 0}^{n} \left( f[x_0, x_1, \ldots, x_j] \prod_{i = 0}^{j - 1} (x - x_i) \right) {.} 
		\end{align*}
		
		% % k % %
		\item Newton's method is adaptive in that we do not need to know all data points, or determine the degree $n$, ahead of time. This is quite helpful, for example, in laboratory measurements where not all data become available at once. 
		
		% % l % %
		\item The linear system solved for in the monomial basis involves an $n \times n$ coefficient matrix which is invertible with entries $x_i^j$ for $i, j = 0, 1, \ldots, n$. 
		
		The linear system solved for in the Lagrange representation involves an $n \times n$ identity matrix, since the matrix elements are $\phi_j(x_i) =  L_j(x_i)$, which will equal 0 when $i \neq j$ and 1 when $i = j$, as defined in (f). The system thus yields the solution $c_j = y_j$ for $j = 0, 1, \ldots, n$. 
		
		The linear system solved for in the Newton representation involves an $n \times n$ lower triangular matrix with nonzero entries along the diagonal, and hence is invertible. The entries are given by $\gamma_{j, l} = f[x_{j - l}, x_{j - l + 1}, \ldots x_j]$, $0 \leq l \leq j \leq n$. Additionally, there is a unique solution for the unknown coefficients $c_0, \ldots, c_n$ in terms of the data $y_0, \ldots, y_n$. It is more or less the divided difference table. 
		
		% % m % %
		\item This connection is illustrated in the following theorem. 
		
		\begin{theorem*}
			Let the function $f$ be defined and have $k$ bounded derivatives in an interval $[a, b]$ and let $z_0, z_1, \ldots, z_k$ be $k + 1$ distinct points in $[a, b]$. Then there is a point $\zeta \in [a, b]$ such that 
			
			\[
			f[z_0, z_1, \ldots, z_k] = \frac{f^{(k)}(\zeta)}{k!} {.}
			\]
		\end{theorem*}
		
		% % n % %
		\item We define the \emph{error function} of a constructed interpolant as 
		
		\[
		e_n(x) = f(x) - p_n(x) = \frac{f^{(n + 1)} (\xi)}{(n + 1)!} \prod_{i = 0}^n (x - x_i) {,}
		\]
		
		where the right-hand side makes sense if $f$ is smooth enough; $f$ having $n + 1$ bounded derivatives is sufficient. An expression for the error bound is given by
		
		\[
		\max_{a \leq x \leq b} |f(x) - p_n(x)| \leq \frac{1}{(n + 1)!} \max_{a \leq t \leq b} |f^{(n + 1)}(t)| \max_{a \leq s \leq b} \prod_{i = 0}^{n} |s - x_i| {.}
		\]
		
		% % o % %
		\item 
		
		% % p % %
		\item 
		
		% % q % %
		\item 
		
		% % r % %
		\item 
		
		% % s % %
		\item 
		
		% % t % %
		\item 
	\end{enumerate}
\end{proof}

% % % % % % % % % % %
% % % Exercise 1 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 2 % % %
% % % % % % % % % % %
\begin{exercise}
	 
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 3 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 4 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 5 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 6 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 7 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

\end{document}