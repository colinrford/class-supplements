\documentclass[12pt,a4]{article}

\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{amscd}
\usepackage{mathtools}
\usepackage{geometry, algorithmicx} 
\usepackage[noend]{algpseudocode}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage{pgf, tikz}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{arrows, automata}
\theoremstyle{definition}


\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[CE]{\Author}
\fancyhead[CO]{\Title}
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}

\author{Colin Ford}
\title{Ascher - Chapter 10 Exercises}
\date{}

\makeatletter
\let\Title\@title
\makeatother

\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem{problem}{Problem}
\newtheorem*{problem*}{Problem}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem*{definition*}{Definition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}
\newtheorem*{example}{Example}

\setcounter{exercise}{-1}

\begin{document}

\maketitle

% % % % % % % % % % %
% % % Exercise 0 % % %
% % % % % % % % % % %
\begin{exercise}[Review Questions]
	\begin{enumerate}[(a)]
		% % a % %
		\item Distinguish between the terms data fitting, interpolation, and polynomial interpolation.
		
		% % b % %
		\item Distinguish between (discrete) data fitting and approximating a given function.
		
		% % c % %
		\item What are basis functions? Does an approximant $v(x)$ that is written as a linear combination of basis functions have to be linear in $x$?
		
		% % d % %
		\item An interpolating polynomial is unique regardless of the choice of the basis. Explain why.
		
		% % e % %
		\item State one advantage and two disadvantages of using the monomial basis for polynomial interpolation.
		
		% % f % %
		\item What are Lagrange polynomials? How are they used for polynomial interpolation?
		
		% % g % %
		\item What are barycentric weights?
		
		% % h % %
		\item State the main advantages and the main disadvantage for using the Lagrange representation.

		% % i % %
		\item What is a divided difference table and how is it constructed?
		
		% % j % %
		\item Write down the formula for polynomial interpolation in Newton form.
		
		% % k % %
		\item State two advantages and two disadvantages for using the Newton representation for polynomial interpolation.
		
		% % l % %
		\item Describe the linear systems that are solved for the monomial basis, the Lagrange representation, and the Newton representation.
		
		% % m % %
		\item Describe the connection between the $k$th divided difference of a function $f$ and its $k$th derivative.
		
		% % n % %
		\item Provide an expression for the error in polynomial interpolation as well as an error bound expression.
		
		% % o % %
		\item How does the smoothness of a function and its derivatives affect the quality of polynomial interpolants that approximate it, in general?
		
		% % p % %
		\item Give an example where the error bound is attained. 
		
		% % q % %
		\item When we interpolate a function $f$ given only data points, i.e., we do not know $f$ or its derivatives, how can we gauge the accuracy of our approximation?
		
		% % r % %
		\item What are Chebyshev points and why are they important?
		
		% % s % %
		\item Describe osculating interpolation. How is it different from the usual polynomial interpolation?
		
		% % t % %
		\item What is a Hermite cubic interpolant?
		
	\end{enumerate}
\end{exercise}
\begin{proof}[Solution]
	\begin{enumerate}[(a)]
		% % a % %
		\item \emph{Data fitting} is the process of finding or constructing a reasonable function $v$ that fits a given set of data. \emph{Interpolation} is a method of constructing a reasonable function $v$ to fit data such that this function exactly passes through the given data. In other words, given a set of data points $\{ (x_i, y_i) \}_{i = 0}^n$, require that $v(x)$ \emph{interpolate} this data so that it satisfies 
		
		\[
		v(x_i) = y_i {,} \quad i = 0, 1 \ldots, n {.}
		\]		
		
		From this one can understand better why interpolation is a special case of approximation. \emph{Polynomial interpolation} is a special case of interpolation, where we restrict our search for an interpolated function $v$ to the polynomials, i.e., 
		
		\[
		v(x) = \sum_{j = 0}^{n} c_j x^j = c_0 + c_1 x^1 + \cdots + c_n x^n {.}
		\]
		
		By ``reasonable'', we mean (loosely) that the interpolating function must resemble a curve that we would actually draw through the points. By ``restrict our search'', we simply mean to restrict our choice of basis functions to some set of polynomials. 
		
		% % b % %
		\item As stated in (a), discrete data fitting is the process of, given a set of (discrete) data points $\{ (x_i, y_i) \}_{i = 0}^n$, finding a reasonable function $v(x)$ that fits the data points. Approximating a given function\footnote{Note that this given function could be given explicitly or implicitly.} $f(x)$ is the process of finding some simpler function $v(x)$ that approximates $f(x)$. The difference between these two methods is that, in approximating a function, we have some freedom of choosing the $x_i$ cleverly as well as that we may be able to consider the \emph{global} interpolation error. 
		
		It is easy to see how one may have more opportunity to consider the global interpolation error when approximating some given function since we may use global properties of the given function when approximating. On the other hand, when data fitting, we are merely given a set of data points which will allow us to construct a function which only \emph{locally} resembles the approximated data or function. 
		
		% % c % %
		\item If $V$ is a vector space over a field $k$, a \emph{linear form} $f$ is a linear function from $V$ to $k$, i.e.
		
		\begin{gather*}
		f(u + v) = f(u) + f(v) \quad \text{for all } u, v \in V \\
		f(a v) = a f(v) \quad \text{for all } a \in k \text{ and } v \in V {.}
		\end{gather*}
		
		We generally assume a linear form for all interpolating functions $v(x)$, and write 
		
		\[
		v(x) = \sum_{j = 0}^n c_j \phi_j(x) = c_0 \phi_0(x) + \cdots + c_n \phi_n(x) {,}
		\]
		
		where $\{ c_j \}_{j = 0}^n$ are \emph{unknown coefficients} or \emph{parameters} determined from the data and $\{ \phi_j (x) \}_{j  = 0}^n$ are predetermined \emph{basis functions}. These basis functions are assumed to be linearly independent. Notice that $v(x)$ is a linear form, so it is not necessarily linear in $x$; it is linear in its basis functions. This is a simple consequence of the definition. 
		
		% % d % %
		\item We give simple proof of the following theorem. 
		
		\begin{theorem*}
			Let $\{ x_i \}_{i = 0}^n$ be a sequence of distinct real numbers. Then for some arbitrary sequence $\{ y_i \}_{i = 0}^n$ of real numbers there exists a unique polynomial $p$ of degree $n$ such that 
			
			\[
			p(x_i) = y_i \quad \text{for } i = 0, \ldots, n {.}
			\]
		\end{theorem*}
		\begin{proof}
			Suppose there were another such polynomial $q$. Then the polynomial $p - q$ is of degree $n$ with zeros at $x_i$ for each $i$. Hence $p - q$ has $n + 1$ zeros, which is only possible if it is the zero polynomial. Hence $p \equiv q$. 
		\end{proof}
		
		% % e % %
		\item A major advantage of using a monomial basis is its intuitive simplicity and straightforwardness. Some disadvantages include: 
		
		\begin{itemize}
			\item the calculated coefficients $c_j$ are not directly indicative of the interpolated function, and they may completely change if we wish to slightly modify the interpolation problem
			
			\item the Vandermonde matrix $X$ is often ill-conditioned, so the coefficients thus determined are prone to inaccuracies 
			
			\item this approach requires about $\frac{2}{3} n^3$ operations (flops) to carry out Gaussian elimination for the construction stage; another method exists which requires only $O(n^2)$ operations. The evaluation stage, however, is as quick as can be; using the nested form, it requires about $2 n$ flops per evaluation point. 
		\end{itemize} 
		
		The latter two disadvantages are not always important. The $O(n^3)$ cost matters when $n$ is large. The ill-conditioning of the Vandermonde matrix matters mostly when the interval of interpolation or the size of $n$ is large. 
		
		% % f % %
		\item A \emph{Lagrange polynomial} $L_j(x)$ is a polynomial of degree $n$ that satisfies
		
		\[
		L_j(x_i) = \begin{cases}
		0 {,} &\quad i \neq j {,} \\
		1 {,} &\quad i = j {.}
		\end{cases}
		\]
		
		Lagrange polynomials are used as a basis in polynomial interpolation. By this we mean 
		
		\[
		p(x) = \sum_{j = 0}^{n} y_j L_j(x) {.} 
		\]
		
		% % g % %
		\item Barycentric weights are used in the construction process of Lagrange polynomial interpolation. Define
		
		\[
		\rho_j = \prod_{i \neq j} (x_j - x_i) {,} \quad w_j = \frac{1}{\rho_j} {,} \quad j = 0, 1, \ldots, n {.}
		\]
		
		This construction requires $O(n^2)$ flops, and the quantities $w_j$ are called \emph{barycentric weights}. 
		
		% % h % %
		\item The Lagrange polynomials form an ideally conditioned basis for all polynomials of degree at most $n$. Another advantage is that the Lagrange representation is easy to manipulate when seeking formulas for differentiation and integration. A disadvantage is that must introduce the interpolation data $(x_i, y_i)$ all at once instead of one pair at a time. 
		
		% % i % %
		\item The coefficient $c_j$ of the interpolating polynomial in Newton's form is called the $j$th \emph{divided difference}, denoted $f[x_0, x_1, \ldots, x_j]$. These are defined recursively as follows. Given points $x_0, x_1, \ldots, x_n$, for arbitrary indices $0 \leq i < j \leq n$, set 
		
		\begin{align*}
		f[x_i] &= f(x_i) {,} \\
		f[x_i, \ldots, x_j] &= \frac{f[x_{i + 1}, \ldots, x_j] - f[x_i, \ldots, x_{j - 1}]}{x_j - x_i} {.}
		\end{align*}
		
		A \emph{divided difference table} is a lower triangular matrix composed of the divided differences, where the diagonal entries are the coefficients $c_j = \gamma_{j, j} = f[x_0, \ldots, x_j]$. 
		
		% % j % %
		\item The formula is given by 
		
		\begin{align*}
		p_n(x) &= f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0) (x - x_1) \\
		 &\quad + \cdots + f[x_0, x_1, \ldots, x_n] (x - x_0) (x - x_1) \cdots (x - x_{n - 1}) \\
		 &= \sum_{j = 0}^{n} \left( f[x_0, x_1, \ldots, x_j] \prod_{i = 0}^{j - 1} (x - x_i) \right) {.} 
		\end{align*}
		
		% % k % %
		\item Newton's method is adaptive in that we do not need to know all data points, or determine the degree $n$, ahead of time. This is quite helpful, for example, in laboratory measurements where not all data become available at once. 
		
		% % l % %
		\item The linear system solved for in the monomial basis involves an $n \times n$ coefficient matrix which is invertible with entries $x_i^j$ for $i, j = 0, 1, \ldots, n$. 
		
		The linear system solved for in the Lagrange representation involves an $n \times n$ identity matrix, since the matrix elements are $\phi_j(x_i) =  L_j(x_i)$, which will equal 0 when $i \neq j$ and 1 when $i = j$, as defined in (f). The system thus yields the solution $c_j = y_j$ for $j = 0, 1, \ldots, n$. 
		
		The linear system solved for in the Newton representation involves an $n \times n$ lower triangular matrix with nonzero entries along the diagonal, and hence is invertible. The entries are given by $\gamma_{j, l} = f[x_{j - l}, x_{j - l + 1}, \ldots x_j]$, $0 \leq l \leq j \leq n$. Additionally, there is a unique solution for the unknown coefficients $c_0, \ldots, c_n$ in terms of the data $y_0, \ldots, y_n$. It is more or less the divided difference table. 
		
		% % m % %
		\item This connection is illustrated in the following theorem. 
		
		\begin{theorem*}
			Let the function $f$ be defined and have $k$ bounded derivatives in an interval $[a, b]$ and let $z_0, z_1, \ldots, z_k$ be $k + 1$ distinct points in $[a, b]$. Then there is a point $\zeta \in [a, b]$ such that 
			
			\[
			f[z_0, z_1, \ldots, z_k] = \frac{f^{(k)}(\zeta)}{k!} {.}
			\]
		\end{theorem*}
		
		% % n % %
		\item We define the \emph{error function} of a constructed interpolant as 
		
		\[
		e_n(x) = f(x) - p_n(x) = \frac{f^{(n + 1)} (\xi)}{(n + 1)!} \prod_{i = 0}^n (x - x_i) {,}
		\]
		
		where the right-hand side makes sense if $f$ is smooth enough; $f$ having $n + 1$ bounded derivatives is sufficient. An expression for the error bound is given by
		
		\[
		\max_{a \leq x \leq b} |f(x) - p_n(x)| \leq \frac{1}{(n + 1)!} \max_{a \leq t \leq b} |f^{(n + 1)}(t)| \max_{a \leq s \leq b} \prod_{i = 0}^{n} |s - x_i| {.}
		\]
		
		% % o % %
		\item The smoother the function and its derivatives, the better the polynomial interpolates that approximate it. 
		
		% % p % %
		\item 
		
		% % q % %
		\item We can gauge the accuracy of an approximation by computing a sequence of polynomials $p_k(x), p_{k + 1}(x), \ldots$ using different subsets of the points and comparing results to see how well they agree. 
		
		% % r % %
		\item 
		
		% % s % %
		\item Consider the \emph{osculating polynomial} of lowest degree satisfying
		
		\[
		p_n^{(k)} (t_i) = f^{(k)} (t_i) \quad (k = 0, \ldots, m_i), \ i = 0, 1, \ldots, q {.}
		\]
		
		Osculating interpolation generalizes the usual polynomial interpolation by interpolating the function as well as its derivatives. 
		
		% % t % %
		\item The most popular osculating polynomial interpolant, it is obtained by setting $q = m_0 = m_1 = 1$. 
	\end{enumerate}
\end{proof}

% % % % % % % % % % %
% % % Exercise 1 % % %
% % % % % % % % % % %
\begin{exercise}
	Derive the linear interpolant through the two data points $(1.0, 2.0)$ and $(1.1., 2.5)$. Derive also the quadratic interpolant through these two pairs as well as $(1.2, 1.5)$. Show that the situation can be depicted as in Figure 10.11. 
\end{exercise}
\begin{proof}[Solution]
	We will use the method of monomial interpolation. Hence for the linear interpolant we want to fit a polynomial of degree at most 1 of the form 
	
	\[
	p_1(x) = c_0 + c_1 x 
	\]
	
	\noindent through these two points. The interpolating conditions are 
	
	\begin{align*}
	p_1(1) &= c_0 + c_1(1) = 2.0 \\
	p_1(1.1) &= c_0 + c_1(1.1) = 2.5 {,} \\
	\end{align*}
	
	\noindent and after a straightforward computation we find that $c_1 = 5$ and $c_0 = -3$, giving 
	
	\[
	p_1(x) = 5 x - 3 {.}
	\]
	
	\noindent Next let $n = 2$ and let our 3 data points be the two before as well as $(1.2, 1.5)$. We want to fit a polynomial of degree at most 2  of the form 
	
	\[
	p_2(x) = c_0 + c_1 x + c_2 x^2
	\]
	
	\noindent through the above points. The interpolating conditions are 
	
	\begin{align*}
	p_2(1) &= c_0 + c_1 (1) + c_2 (1)^2 = 2 \\
	p_2(1.1) &= c_0 + c_1 (1.1) + c_2 (1.1)^2 = 2.5 \\
	p_2(1.2) &= c_0 + c_1 (1.2) + c_2 (1.2)^2 = 1.5 \\
	\end{align*}
	
	\noindent which gives us a $3 \times 3$ linear system for the unknowns $c_j$. Solving this system yields $c_2 = -75$, $c_1 = 162.5$ and $c_0 = -85.5$.
\end{proof}

% % % % % % % % % % %
% % % Exercise 2 % % %
% % % % % % % % % % %
\begin{exercise}
	Some modeling considerations have mandated a search for a function 
	
	\[
	u(x) = \gamma_0 e^{\gamma_1 x + \gamma_2 x^2} {,}
	\]
	
	\noindent where the unknown coefficients $\gamma_1$ and $\gamma_2$ are expected to be nonpositive. Given are data pairs to be interpolated, $(x_0, z_0)$, $(x_1, z_1)$ and $(x_2, z_2)$, where $z_i > 0$, $i = 0, 1, 2$. Thus we require $u(x_i) = z_i$. 
	
	The function $u(x)$ is not linear in its coefficients, but $v(x) = \ln(u(x))$ is linear in its. 
	
	\begin{enumerate}[(a)]
		% % a % %
		\item Find a quadratic polynomial $v(x)$ that interpolates appropriately defined three data pairs, and then give a formula for $u(x)$ in terms of the original data. 
		
		% % b % %
		\item Write a script to find $u$ for the data $(0, 1)$, $(1, .9)$, $(3, .5)$. Give the coefficients $\gamma_i$ and plot the resulting interpolant over the interval $[0, 6]$. In what way does the curve behave qualitatively differently from a quadratic?
	\end{enumerate}
\end{exercise}
\begin{proof}[Solution]
	\begin{enumerate}[(a)]
		% % a % %
		\item Consider three pairs $(x_0, z_0)$, $(x_1, z_1)$ and $(x_2, z_2)$, where $z_i > 0$ for each $i$. We want to find some quadratic polynomial $v(x) = \ln(u(x))$ such that $v(x)$ interpolates the points $(x_0, y_0)$, $(x_1, y_1)$ and $(x_2, y_2)$ where $y_i = \ln(z_i)$ for each $i$. Note that
		
		\[
		v(x) = \gamma_0' + \gamma_1 x + \gamma_2 x^2 {,} \quad \text{where } \gamma_0' = \ln{\gamma_0} {.}
		\]
		
		We use Lagrange interpolation here as it is the simplest in finding the coefficients $\gamma_i$. Consider the Lagrange polynomials 
		
		\begin{align*}
		L_0(x) &= a (x - x_1) (x - x_2) \\
		L_1(x) &= b (x - x_0) (x - x_2) \\
		L_2(x) &= c (x - x_0) (x - x_1) {.} \\
		\end{align*}
		
		So $a = 1 / ((x_0 - x_1)(x_0 - x_2))$, $b = 1 / ((x_1 - x_0)(x_1 - x_2))$ and $c = 1 / ((x_2 - x_0)(x_2 - x_1))$. Hence, after rearranging, 
		
		\begin{gather*}
		v(x) = (a y_0 + b y_1 + c y_2) x^2 + (a y_0 (x_1 + x_2) + b y_1 (x_0 + x_2) + c y_2 (x_0 + x_1)) x \\
		a y_0 x_1 x_2 + b y_1 x_0 x_2 + c y_2 x_0 x_1 {.}
		\end{gather*}
		
		By the definition of $v$, we have that $u(x) = e^{v(x)}$, hence 
		
		\begin{gather*}
		u(x) = \exp{(a y_0 x_1 x_2 + b y_1 x_0 x_2 + c y_2 x_0 x_1)} \\
		 \cdot \exp{((a y_0 + b y_1 + c y_2) x^2)} \\
		 \cdot \exp{(-(a y_0 (x_1 + x_2) + b y_1 (x_0 + x_2) + c y_2 (x_0 + x_1)) x)} {.}
		\end{gather*}
		
		% % b % %
		\item 
	\end{enumerate}
\end{proof}

% % % % % % % % % % %
% % % Exercise 3 % % %
% % % % % % % % % % %
\begin{exercise}
	Use the known values of the function $\sin(x)$ at $x = 0, \frac{\pi}{6}, \frac{\pi}{4}, \frac{\pi}{3}$ and $\frac{\pi}{2}$ to derive an interpolating polynomial $p(x)$. What is the degree of your polynomial?
	
	\noindent What is the interpolation error magnitude $|p(1.2) - \sin(1.2)|$?
\end{exercise}
\begin{proof}[Solution]
	The degree of this polynomial is 4. (Computation too tedious for now; use Lagrange interpolation)
\end{proof}

% % % % % % % % % % %
% % % Exercise 4 % % %
% % % % % % % % % % %
\begin{exercise}
	Given $n + 1$  data pairs $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$, define for $j = 0, 1 \ldots, n$ the functions $\rho_j = \prod_{i \neq j} (x_j - x_i)$, and let also $\psi(x) = \prod_{i = 0}^n (x - x_i)$. 
	
	\begin{enumerate}[(a)]
		% % a % % 
		\item Show that 
		
		\[
		\rho_j = \psi'(x_j) {.}
		\]
		
		% % b % %
		\item Show that the interpolating polynomial of degree at most $n$ is given by 
		
		\[
		p_n(x) = \psi(x) \sum_{j = 0}^{n} \frac{y_j}{(x - x_j)\psi'(x_j)} {.}
		\]
	\end{enumerate}
\end{exercise}
\begin{proof}[Solution]
	\begin{enumerate}[(a)]
		% % a % % 
		\item This follows by an application of the product rule. We have
		
		\[
		\psi'(x) = \sum_{k = 0}^n \prod_{i \neq k} (x - x_i) {,} \tag{4.1} \label{eq:4.1}
		\]
		
		and if we evaluate $\psi'$ at $x = x_j$, we notice that \hyperref[eq:4.1]{\eqref{eq:4.1}} reduces to $\rho_j$. To see this, notice that 
		
		\begin{align*}
		\psi'(x_j) &= \sum_{k = 0}^n \prod_{i \neq k} (x_j - x_i) \\
		 &= \prod_{i \neq j} (x_j - x_i) {,}
		\end{align*}
		
		since when $k \neq j$, $\prod_{i \neq k} (x_j - x_i) = 0$ since $i = j$ at some point in the product. 
		
		% % b % %
		\item This follows from the definition of Lagrange interpolation. Recall that 
		
		\[
		p_n(x) = \sum_{j = 0}^n y_j L_j(x) = \sum_{j = 0}^n y_j \underset{i \neq j}{\prod_{i = 0}^n} \frac{x - x_i}{x_j - x_i} {.} \tag{4.2} \label{eq:4.2}
		\]
		
		If we multiply the RHS of \hyperref[eq:4.2]{\eqref{eq:4.2}} by $(x - x_j) / (x - x_j)$, we have
		
		\begin{align*}
		p_n(x) &= \sum_{j = 0}^n y_j \frac{\prod_{i = 0}^n (x - x_i)}{(x - x_j) \prod_{i \neq j} (x_j - x_i)} \\
		&= \prod_{i = 0}^n (x - x_i) \sum_{j = 0}^n \frac{y_j}{(x - x_j) \psi'(x_j)} \tag{using (a)} \\
		&= \psi(x) \sum_{j = 0}^n \frac{y_j}{(x - x_j) \psi'(x_j)} {.}
		\end{align*}
		
		
	\end{enumerate}
\end{proof}

% % % % % % % % % % %
% % % Exercise 5 % % %
% % % % % % % % % % %
\begin{exercise}
	Construct a polynomial $p_3(t)$ of degree at most three in \emph{Lagrange form} that interpolates the data 
	
	\[
	\begin{tabular}{|c|c|c|c|c|}
	$t$ & -1.1 & 0.0 & 1.1 & 2.2 \\
	\hline
	$y$ & 0.0 & 0.0 & 6.75 & 0.0 \\
	\end{tabular}
	\]
\end{exercise}
\begin{proof}[Solution]
	Notice that $y_0 = y_1 = y_3 = 0$, so we should expect a relatively simple calculation. Using the Lagrange form, we have
	
	\begin{align*}
	p_3(t) &= \sum_{j = 0}^3 y_j L_j(x) \\
	 &= y_2 \frac{(x - x_0) (x - x_1) (x - x_3)}{(x_2 - x_0) (x_2 - x_1) (x_2 - x_3)} \\
	 &= 6.75 \frac{x (x + 1.1) (x - 2.2)}{1.1 (1.1 + 1.1) (1.1 - 2.2)} \\
	 &= 6.75 \frac{x^3 - 1.1 x^2 - 2.42 x}{-2.662} \\
	 &= \frac{6.75 x^3 - 7.425 x^2 - 16.335 x}{-2.662} {.}
	\end{align*}
	
	We kept the fraction for a more exact form. 
\end{proof}

% % % % % % % % % % %
% % % Exercise 6 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 7 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 8 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 9 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 10 % % %
% % % % % % % % % % %
\begin{exercise}
	(later)
\end{exercise}

% % % % % % % % % % %
% % % Exercise 11 % % %
% % % % % % % % % % %
\begin{exercise}
	(later)
\end{exercise}

% % % % % % % % % % %
% % % Exercise 12 % % %
% % % % % % % % % % %
\begin{exercise}
	(later)
\end{exercise}

% % % % % % % % % % %
% % % Exercise 13 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 14 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 15 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 16 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 17 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 18 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 19 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 20 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 21 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}

% % % % % % % % % % %
% % % Exercise 22 % % %
% % % % % % % % % % %
\begin{exercise}
	
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

% % % % % % % % % % %
% % % Exercise 23 % % %
% % % % % % % % % % %
\begin{exercise}
	(later) 
\end{exercise}
\begin{proof}[Solution]
	
\end{proof}

\end{document}